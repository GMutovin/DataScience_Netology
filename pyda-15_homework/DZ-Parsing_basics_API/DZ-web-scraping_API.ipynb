{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание к лекции \"Основы веб-скрапинга и работы с API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Будем парсить страницу со свежеми новостям на habr.com/ru/all/.\n",
    "\n",
    "# Вам необходимо собирать только те статьи, в которых встречается хотя бы одно требуемое ключевое слово. \n",
    "# Эти слова определяем в начале кода в переменной, например:\n",
    "\n",
    "# KEYWORDS = ['python', 'парсинг']\n",
    "\n",
    "# Поиск вести по всей доступной preview-информации (это информация, доступная непосредственно с текущей страницы).\n",
    "\n",
    "# В итоге должен формироваться датафрейм вида: <дата> - <заголовок> - <ссылка>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = ['Selectel', 'производительность']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://habr.com/ru/all/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://habr.com/ru/all/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = soup.find_all('article', class_='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_titles = []\n",
    "post_dates = []\n",
    "post_links = []\n",
    "\n",
    "for post in posts:\n",
    "    # Убираем не посты\n",
    "    post_id = post.parent.attrs.get('id')\n",
    "    if not post_id:\n",
    "        continue\n",
    "    # Находим текстовую часть поста\n",
    "    texts = post.find_all('div', class_='post__text')\n",
    "    # Проходим циклом непосредственно по содержанию превью поста\n",
    "    for text_body in texts:\n",
    "        text_body = text_body.text\n",
    "        if any([desired in text_body for desired in KEYWORDS]):\n",
    "            # Находим дату и время поста и добавляем в список релевантных постов\n",
    "            post_date = post.find('span', class_='post__time')\n",
    "            post_dates.append(post_date.text)\n",
    "            # Находим заголовок поста и добавляем в список релевантных постов\n",
    "            post_title = post.find('h2', class_='post__title')\n",
    "            post_titles.append(post_title.text.strip())\n",
    "            # Находим ссылку на пост и добавляем в список релевантных постов\n",
    "            post_link = post.find('a', class_='post__title_link')\n",
    "            post_links.append(post_link.attrs.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем датафрейм вида: <дата> - <заголовок> - <ссылка>\n",
    "habr_new_posts = pd.DataFrame(\n",
    "    {\n",
    "        'дата': post_dates,\n",
    "        'заголовок': post_titles,\n",
    "        'ссылка': post_links\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дополнительная часть (необязательная)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Улучшить скрипт так, чтобы он анализировал не только preview-информацию статьи, но и весь текст статьи целиком.\n",
    "\n",
    "# Для этого потребуется получать страницы статей и искать по тексту внутри этой страницы.\n",
    "\n",
    "# Итоговый датафрейм формировать со столбцами: <дата> - <заголовок> - <ссылка> - <текст_статьи>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(url):\n",
    "    all_refs = []\n",
    "    res = requests.get(url)\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    hub_blocks = soup.find_all('article', class_='post')\n",
    "    all_refs = list(map(lambda x: x.find('a', class_='post__title_link').get('href'), hub_blocks))\n",
    "    \n",
    "    return all_refs\n",
    "\n",
    "all_links = get_all_links(URL)\n",
    "all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_titles = []\n",
    "post_dates = []\n",
    "post_links = []\n",
    "post_texts = []\n",
    "\n",
    "for link in all_links:\n",
    "    soup = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "    time.sleep(0.3)\n",
    "    text_body = soup.find('div', class_='post__text')\n",
    "    if any([desired in text_body.text for desired in KEYWORDS]):\n",
    "        # Находим дату и время поста и добавляем в список релевантных постов\n",
    "        post_date = soup.find('span', class_='post__time')\n",
    "        post_dates.append(post_date.text)\n",
    "        # Находим заголовок поста и добавляем в список релевантных постов\n",
    "        post_title = soup.find('span', class_='post__title-text')\n",
    "        post_titles.append(post_title.text.strip())\n",
    "        # Добавляем ссылку в список релевантных постов\n",
    "        post_links.append(link)\n",
    "        # Берем весь текст статьи\n",
    "        post_texts.append(text_body.text.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оформляем результаты в датафрейм - в каждой строке будут прибавляться последовательно элементы \n",
    "#соответствующих списков, в которые были включены результаты скрапинга - post_titles, post_dates, post_links и post_texts\n",
    "habr_new_posts_amd = pd.DataFrame(\n",
    "    {\n",
    "        'дата': post_dates,\n",
    "        'заголовок': post_titles,\n",
    "        'ссылка': post_links,\n",
    "        'текст_статьи': post_texts\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Написать скрипт, который будет проверять список e-mail адресов на утечку при помощи сервиса Avast Hack Ckeck. \n",
    "# Список email-ов задаем переменной в начале кода:\n",
    "# EMAIL = [xxx@x.ru, yyy@y.com]\n",
    "\n",
    "# В итоге должен формироваться датафрейм со столбцами: <почта> - <дата утечки> - <источник утечки> - <описание утечки>\n",
    "\n",
    "# Подсказка: сервис работает при помощи \"скрытого\" API. Внимательно изучите post-запросы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные, которые будут передаваться в POST запрос\n",
    "EMAIL = ['xxx@x.ru', 'yyy@y.com']\n",
    "URL = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data'\n",
    "headers = {\n",
    "    'Vaar-Version': '0', \n",
    "    'Vaar-Header-App-Product': 'hackcheck-web-avast'\n",
    "}\n",
    "payload = {'emailAddresses': EMAIL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делаем POST запрос к \"скрытому\" API\n",
    "response = requests.post(URL, json=payload, headers=headers)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем датафрейм с информацией по \"утечкам\": дата утечки, источник утечки, описание утечки и id утечки \n",
    "# id утечки нужен для последующего объединения с датафреймом по почтам\n",
    "emails_leaks = pd.DataFrame()\n",
    "\n",
    "for breach, descr in response.json()['breaches'].items():\n",
    "    date = pd.to_datetime(descr['publishDate'], dayfirst=True).date()\n",
    "    row = {'breach_id': int(breach), 'дата утечки': date, 'источник утечки': descr['site'], 'описание утечки': descr['description']}\n",
    "    emails_leaks = pd.concat([emails_leaks, pd.DataFrame([row])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем датафрейм с информацией по \"email\": id утечки и соответствующий email\n",
    "emails_ids = pd.DataFrame()\n",
    "\n",
    "for email, value in response.json()['summary'].items():\n",
    "    for breach in value['breaches']:\n",
    "        # Каждую итерацию добавляем 'breach_id' и соответствующий email\n",
    "        row = {'breach_id': int(breach), 'emails': email}\n",
    "        # Переводим row в датафрейм и присоединяем его к созданному датафрейму \"emails_ids\"\n",
    "        emails_ids = pd.concat([emails_ids, pd.DataFrame([row])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем датафреймы - к emails_ids присоединяем emails_leaks по столбцу \"breach_id\"\n",
    "merged_email_leak_info = emails_ids.merge(emails_leaks, how='left', on='breach_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем столбец \"breach_id\"\n",
    "merged_email_leak_info.drop(['breach_id'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дополнительная часть (необязательная)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Написать скрипт, который будет получать 50 последних постов указанной группы во Вконтакте.\n",
    "# Документация к API VK: https://vk.com/dev/methods , вам поможет метод wall.get\n",
    "\n",
    "# GROUP = 'netology'  \n",
    "# TOKEN = УДАЛЯЙТЕ В ВЕРСИИ ДЛЯ ПРОВЕРКИ, НА GITHUB НЕ ВЫКЛАДЫВАТЬ\n",
    "# В итоге должен формироваться датафрейм со столбцами: <дата поста> - <текст поста>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Адрес API вконтакте\n",
    "POSTS_FEED = 'https://api.vk.com/method/wall.get?'\n",
    "# Параметры запроса\n",
    "GROUP = 'netology'\n",
    "TOKEN = 'XXXXX'\n",
    "VERSION = 5.126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры запроса к API\n",
    "params = {\n",
    "    'domain': GROUP,\n",
    "    'access_token': TOKEN,\n",
    "    'v': VERSION,\n",
    "    'count': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запрос к API вконтакте, по которому получаем 50 последних записей на стене группы \"Нетология\"\n",
    "res = requests.get(POSTS_FEED, params)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем датафрейм с объектами результата запроса\n",
    "df = pd.DataFrame(res.json()['response']['items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбрасываем ненужные данные и оставляем только date и text\n",
    "df.drop(['id', 'from_id', 'owner_id', 'marked_as_ads', 'post_type', 'is_pinned',\n",
    "         'attachments', 'comments', 'likes', 'reposts', 'views', 'is_favorite', 'donut',\n",
    "         'short_text_rate', 'carousel_offset', 'edited'\n",
    "        ], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переименовываеми колонки \"date\" и \"text\" в \"дата поста\" и \"текст поста\" соответственно\n",
    "df.columns = ['дата поста', 'текст поста']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для перевода даты поста из формата unixtime в формат дд-мм-гггг\n",
    "def date_(row):\n",
    "    date = datetime.fromtimestamp(row).strftime('%d-%m-%Y')\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переводим дату поста из формата unixtime в формат дд-мм-гггг\n",
    "df['дата поста'] = df['дата поста'].apply(date_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
